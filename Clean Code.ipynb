{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20a8369a",
   "metadata": {},
   "source": [
    "# Retrieve Meta Data from Treaties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "788ef95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import pdftotext\n",
    "import docx2txt\n",
    "from os import listdir\n",
    "import fuzzyset\n",
    "import time\n",
    "import difflib\n",
    "import regex as re\n",
    "\n",
    "#get the meta data - webscraped and saved as csv file\n",
    "df_meta_treaty = pd.read_csv(\"Meta Data Treaties.csv\")\n",
    "df_meta_treaty_decision = pd.read_csv(\"Meta Data Treaty Decisions.csv\")\n",
    "\n",
    "#create list with all treaty names / These names in the list will be searched later\n",
    "treaty_names = list(df_meta_treaty[\"Title\"])\n",
    "\n",
    "#delete fill words\n",
    "for number, treaty in enumerate(treaty_names):\n",
    "    new_name = \"\"\n",
    "    for word in treaty.split():\n",
    "        if (word.lower() in stopwords.words(\"english\")) or (word.lower() in stopwords.words(\"spanish\")): #some documents are spanish\n",
    "            pass\n",
    "        else:\n",
    "            new_name += (word + \" \")\n",
    "    treaty_names[number] = new_name[:-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd2225",
   "metadata": {},
   "source": [
    "# Convert Docx/PDFs to txt files - Can be skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97578dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only for one, has to be automized for all pdfs + create txt file with name list\n",
    "\n",
    "\n",
    "list_of_subfolders = [\"Decisions\", \"Bilateral Agreements\", \"Multilateral Agreements\", \"Official Documents\", \"Other\", \"Recommendations\", \"Resolutions\", \"Treaties\"]\n",
    "\n",
    "\n",
    "#for statistics later\n",
    "successes = 0\n",
    "fails = 0\n",
    "\n",
    "\n",
    "#loop over different subfolders\n",
    "for subfolder in list_of_subfolders:\n",
    "    sub_fails = 0\n",
    "    sub_suc = 0\n",
    "    #get list of all files in one folder\n",
    "    list_of_files = listdir(\"Downloads/\" + subfolder)\n",
    "\n",
    "    #loop over different files\n",
    "    for file in list_of_files:\n",
    "        try: #try to avoid crashing the programm, as some pdfs are corrupted\n",
    "            if file[-3:] == \"pdf\":\n",
    "                with open(f\"Downloads/{subfolder}/{file}\", \"rb\") as f:\n",
    "                    pdf = pdftotext.PDF(f)\n",
    "\n",
    "                with open(f\"Downloads/{subfolder}/{file[:-3]}.txt\", \"w\", encoding='utf-8') as f:\n",
    "                    f.write(\" \".join(pdf))\n",
    "                successes += 1\n",
    "                sub_suc += 1\n",
    "            \n",
    "            if file[-4:] == 'docx':\n",
    "                doc_text = docx2txt.process(f'Downloads/{subfolder}/{file}')\n",
    "                with open(f\"Downloads/{subfolder}/{file[:-4]}.txt\", \"w\", encoding='utf-8') as f:\n",
    "                    f.write(doc_text)\n",
    "                successes += 1\n",
    "                sub_suc += 1\n",
    "                \n",
    "                \n",
    "        except:\n",
    "            fails += 1\n",
    "            sub_fails += 1\n",
    "            \n",
    "    print(\"subfolder \", subfolder, \"done\")\n",
    "    print(f\"All complete in folder {subfolder}, with a success rate of {sub_suc/(sub_fails+sub_suc)} on {sub_suc + sub_fails} files\")\n",
    "\n",
    "print(\"All complete, with a success rate of \", (successes/(fails + successes), \" on \", successes + fails, \" files\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca848c2b",
   "metadata": {},
   "source": [
    "# Analyze Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d59af7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time FuzzySearch:  201.60133409500122\n",
      "time SequenceMatcher:  91.03226804733276\n"
     ]
    }
   ],
   "source": [
    "list_of_subfolders = [\"Train Data\"] #can be extended, but only use one small folder for now\n",
    "\n",
    "\n",
    "def text_mining(treaty_names):\n",
    "    \n",
    "    #create dataframes for later saving data\n",
    "    df_fuzzysearch = pd.DataFrame(columns = treaty_names)\n",
    "    df_sequencematch = pd.DataFrame(columns = treaty_names)\n",
    "    \n",
    "    \n",
    "    for subfolder in list_of_subfolders: #loop over subfolders\n",
    "        list_of_files = listdir(\"Downloads/\" + subfolder) #get all files\n",
    "\n",
    "        for filename in list_of_files: #go through files in subfolder\n",
    "            \n",
    "            if filename[-3:] == \"txt\":  #check if it is a txt document\n",
    "                with open(\"Downloads/\" + subfolder + \"/\" + filename, encoding = \"utf8\") as f:\n",
    "                    document_text = f.readlines()\n",
    "                    \n",
    "           \n",
    "                #clean the document\n",
    "                text = \" \".join(document_text) #create long string \n",
    "                text = text.lower()\n",
    "                text = re.sub(\"</?.*?>\",\" <> \", text) # remove tags\n",
    "                text = re.sub(\"(\\\\d|\\\\W)+\",\" \", text).strip() # remove special characters and digits\n",
    "                text = text.split()\n",
    "                \n",
    "                \n",
    "                list_of_outcomes = []\n",
    "                start_time = time.time() #to later see the speed of the programm\n",
    "                \n",
    "                #loop over treaty names, to see if it is mention in the document text\n",
    "                for treaty in treaty_names: \n",
    "                    \n",
    "                    #treaty = \"Nagoya Kuala Lumpur Supplementary Protocol Liability Redress Cartagena Protocol Biosafety\"\n",
    "                    \n",
    "                    \n",
    "                    '''\n",
    "                    VERSION 1: Use fuzzy search\n",
    "                    As fuzzysearch doesn't work well if the string length differ widely,\n",
    "                    the code loops over different overlaping parts of the text and saves the highest score\n",
    "                    '''\n",
    "                    high_score = 0\n",
    "                    name_len = len(treaty.split()) #see how long the treaty name is\n",
    "                    for num  in range(0, len(text) - 2 - name_len, (name_len + 2) // 2): #get the length of the text and get a step size that is according to the name length\n",
    "                        \n",
    "                        #use fuzzy search to see the match for the specific part of the text\n",
    "                        Search_Query = fuzzyset.FuzzySet()\n",
    "                        Search_Query.add(treaty)\n",
    "                        outcome = Search_Query.get(\"\".join(text[num:num + name_len + 2]))\n",
    "                        \n",
    "                        \n",
    "                        if outcome != None: #check if the match is None (None will cause error)\n",
    "                            if outcome[0][0] > high_score: #check if the current score is larger than the highest score\n",
    "                                high_score = outcome[0][0] #update highscore\n",
    "                    list_of_outcomes.append(high_score)\n",
    "                \n",
    "                #add outcomes to df, save df\n",
    "                df_fuzzysearch.loc[filename[-3:]] = list_of_outcomes\n",
    "                df_fuzzysearch.to_csv('Downloads/Fuzzy Search Outcomes.csv')\n",
    "                print('time FuzzySearch: ', time.time() - start_time)\n",
    "                \n",
    "                \n",
    "                #print closes match\n",
    "                #print(filename, ' Match: ', treaty_names[list_of_outcomes.index(max(list_of_outcomes))], (max(list_of_outcomes)))\n",
    "                \n",
    "                    \n",
    "                list_of_outcomes = []\n",
    "                start_time = time.time()\n",
    "                for treaty in treaty_names: \n",
    "                    '''\n",
    "                    VERSION 2: SequenceMatcher\n",
    "                    Different library - roughly three times faster\n",
    "                    does not need to loop over \n",
    "                    '''\n",
    "                    \n",
    "                    \n",
    "                    s = difflib.SequenceMatcher(None, ''.join(text), treaty)\n",
    "                    high_score = sum(n for i,j,n in s.get_matching_blocks()) / float(len(treaty))\n",
    "                    list_of_outcomes.append(high_score)\n",
    "                    \n",
    "                #add outcomes to df, save df\n",
    "                print('time SequenceMatcher: ', time.time() - start_time)\n",
    "                df_sequencematch.loc[filename[-3:]] = list_of_outcomes\n",
    "                df_sequencematch.to_csv('Downloads/Seqeunce Match Outcomes.csv')\n",
    "\n",
    "                \n",
    "            \n",
    "                \n",
    "    \n",
    "text_mining(treaty_names)\n",
    "print(\"ENDE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
