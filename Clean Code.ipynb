{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ff1158",
   "metadata": {},
   "source": [
    "# Retrieve Meta Data from Treaties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd2f861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import pdftotext\n",
    "import docx2txt\n",
    "from os import listdir\n",
    "import fuzzyset\n",
    "import time\n",
    "import difflib\n",
    "import regex as re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "import ray \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#get the meta data - webscraped and saved as csv file\n",
    "df_meta_treaty = pd.read_csv(\"Meta Data Treaties.csv\")\n",
    "df_meta_treaty_decision = pd.read_csv(\"Meta Data Treaty Decisions.csv\")\n",
    "\n",
    "#create list with all treaty names / These names in the list will be searched later\n",
    "treaty_names = list(df_meta_treaty[\"Title\"])\n",
    "\n",
    "#delete fill words\n",
    "for number, treaty in enumerate(treaty_names):\n",
    "    new_name = \"\"\n",
    "    for word in treaty.split():\n",
    "        if (word.lower() in stopwords.words(\"english\")) or (word.lower() in stopwords.words(\"spanish\")): #some documents are spanish\n",
    "            pass\n",
    "        else:\n",
    "            new_name += (word + \" \")\n",
    "    treaty_names[number] = new_name[:-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a117c9",
   "metadata": {},
   "source": [
    "# Convert Docx/PDFs to txt files - Can be skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f1508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only for one, has to be automized for all pdfs + create txt file with name list\n",
    "\n",
    "\n",
    "list_of_subfolders = [\"Decisions\", \"Bilateral Agreements\", \"Multilateral Agreements\", \"Official Documents\", \"Other\", \"Recommendations\", \"Resolutions\", \"Treaties\"]\n",
    "\n",
    "\n",
    "#for statistics later\n",
    "successes = 0\n",
    "fails = 0\n",
    "\n",
    "\n",
    "#loop over different subfolders\n",
    "for subfolder in list_of_subfolders:\n",
    "    sub_fails = 0\n",
    "    sub_suc = 0\n",
    "    #get list of all files in one folder\n",
    "    list_of_files = listdir(\"Downloads/\" + subfolder)\n",
    "\n",
    "    #loop over different files\n",
    "    for file in list_of_files:\n",
    "        try: #try to avoid crashing the programm, as some pdfs are corrupted\n",
    "            if file[-3:] == \"pdf\":\n",
    "                with open(f\"Downloads/{subfolder}/{file}\", \"rb\") as f:\n",
    "                    pdf = pdftotext.PDF(f)\n",
    "\n",
    "                with open(f\"Downloads/{subfolder}/{file[:-3]}.txt\", \"w\", encoding='utf-8') as f:\n",
    "                    f.write(\" \".join(pdf))\n",
    "                successes += 1\n",
    "                sub_suc += 1\n",
    "            \n",
    "            if file[-4:] == 'docx':\n",
    "                doc_text = docx2txt.process(f'Downloads/{subfolder}/{file}')\n",
    "                with open(f\"Downloads/{subfolder}/{file[:-4]}.txt\", \"w\", encoding='utf-8') as f:\n",
    "                    f.write(doc_text)\n",
    "                successes += 1\n",
    "                sub_suc += 1\n",
    "                \n",
    "                \n",
    "        except:\n",
    "            fails += 1\n",
    "            sub_fails += 1\n",
    "            \n",
    "    print(\"subfolder \", subfolder, \"done\")\n",
    "    print(f\"All complete in folder {subfolder}, with a success rate of {sub_suc/(sub_fails+sub_suc)} on {sub_suc + sub_fails} files\")\n",
    "\n",
    "print(\"All complete, with a success rate of \", (successes/(fails + successes), \" on \", successes + fails, \" files\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ac9a21",
   "metadata": {},
   "source": [
    "# Analyze Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c019780f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUncomment to run code\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_subfolders = [\"Train Data\"] #can be extended, but only use one small folder for now\n",
    "\n",
    "\n",
    "def text_mining(treaty_names):\n",
    "    \n",
    "    #create dataframes for later saving data\n",
    "    df_fuzzysearch = pd.DataFrame(columns = treaty_names)\n",
    "    df_sequencematch = pd.DataFrame(columns = treaty_names)\n",
    "    \n",
    "    \n",
    "    for subfolder in list_of_subfolders: #loop over subfolders\n",
    "        list_of_files = listdir(\"Downloads/\" + subfolder) #get all files\n",
    "\n",
    "        for filename in list_of_files: #go through files in subfolder\n",
    "            \n",
    "            if filename[-3:] == \"txt\":  #check if it is a txt document\n",
    "                with open(\"Downloads/\" + subfolder + \"/\" + filename, encoding = \"utf8\") as f:\n",
    "                    document_text = f.readlines()\n",
    "                    \n",
    "           \n",
    "                #clean the document\n",
    "                text = \" \".join(document_text) #create long string \n",
    "                text = text.lower()\n",
    "                text = re.sub(\"</?.*?>\",\" <> \", text) # remove tags\n",
    "                text = re.sub(\"(\\\\d|\\\\W)+\",\" \", text).strip() # remove special characters and digits\n",
    "                text = text.split()\n",
    "                \n",
    "                \n",
    "                list_of_outcomes = []\n",
    "                start_time = time.time() #to later see the speed of the programm\n",
    "                \n",
    "                #loop over treaty names, to see if it is mention in the document text\n",
    "                for treaty in treaty_names: \n",
    "                    \n",
    "                    #treaty = \"Nagoya Kuala Lumpur Supplementary Protocol Liability Redress Cartagena Protocol Biosafety\"\n",
    "                    \n",
    "                    \n",
    "                    '''\n",
    "                    VERSION 1: Use fuzzy search\n",
    "                    As fuzzysearch doesn't work well if the string length differ widely,\n",
    "                    the code loops over different overlaping parts of the text and saves the highest score\n",
    "                    '''\n",
    "                    high_score = 0\n",
    "                    name_len = len(treaty.split()) #see how long the treaty name is\n",
    "                    for num  in range(0, len(text) - 2 - name_len, (name_len + 2) // 2): #get the length of the text and get a step size that is according to the name length\n",
    "                        \n",
    "                        #use fuzzy search to see the match for the specific part of the text\n",
    "                        Search_Query = fuzzyset.FuzzySet()\n",
    "                        Search_Query.add(treaty)\n",
    "                        outcome = Search_Query.get(\"\".join(text[num:num + name_len + 2]))\n",
    "                        \n",
    "                        \n",
    "                        if outcome != None: #check if the match is None (None will cause error)\n",
    "                            if outcome[0][0] > high_score: #check if the current score is larger than the highest score\n",
    "                                high_score = outcome[0][0] #update highscore\n",
    "                    list_of_outcomes.append(high_score)\n",
    "                \n",
    "                #add outcomes to df, save df\n",
    "                df_fuzzysearch.loc[filename[-3:]] = list_of_outcomes\n",
    "                df_fuzzysearch.to_csv('Downloads/Fuzzy Search Outcomes.csv')\n",
    "                print('time FuzzySearch: ', time.time() - start_time)\n",
    "                \n",
    "                \n",
    "                #print closes match\n",
    "                #print(filename, ' Match: ', treaty_names[list_of_outcomes.index(max(list_of_outcomes))], (max(list_of_outcomes)))\n",
    "                \n",
    "                    \n",
    "                list_of_outcomes = []\n",
    "                start_time = time.time()\n",
    "                for treaty in treaty_names: \n",
    "                    '''\n",
    "                    VERSION 2: SequenceMatcher\n",
    "                    Different library - roughly three times faster\n",
    "                    does not need to loop over \n",
    "                    '''\n",
    "                    \n",
    "                    \n",
    "                    s = difflib.SequenceMatcher(None, ''.join(text), treaty)\n",
    "                    high_score = sum(n for i,j,n in s.get_matching_blocks()) / float(len(treaty))\n",
    "                    list_of_outcomes.append(high_score)\n",
    "                    \n",
    "                #add outcomes to df, save df\n",
    "                print('time SequenceMatcher: ', time.time() - start_time)\n",
    "                df_sequencematch.loc[filename[-3:]] = list_of_outcomes\n",
    "                df_sequencematch.to_csv('Downloads/Seqeunce Match Outcomes.csv')\n",
    "\n",
    "                \n",
    "            \n",
    "                \n",
    "'''\n",
    "Uncomment to run code\n",
    "'''   \n",
    "#text_mining(treaty_names)\n",
    "#print(\"ENDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a437c708",
   "metadata": {},
   "source": [
    "# Analyze Text in Parallel\n",
    "### roughly 15x faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c69e2396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):          \n",
    "    #clean the document\n",
    "    text = \" \".join(text) #create long string \n",
    "    text = text.lower()\n",
    "    text = re.sub(\"</?.*?>\",\" \", text) # remove tags\n",
    "    text = re.sub('\\(.\\)| .\\)', '', text) #remove a) and (b) mentions\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text).strip() #remove mistranslations such as '-' to 'xe23xo0'; basically remove any words that have numbers in it\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \", text).strip() # remove special characters and digits\n",
    "    return text\n",
    "                \n",
    "            \n",
    "def clean_treaty_names(name):\n",
    "    name = [ n for n in name.split() if len(n)> 3]\n",
    "    name = re.sub(r'\\w*\\d\\w*', '', ' '.join(name)).strip() #remove mistranslations such as '-' to 'xe23xo0'; basically remove any words that have numbers in it\n",
    "    name = re.sub(r'\\w*\\.\\w*', '', name).strip()\n",
    "    name = text = re.sub(\"(\\\\d|\\\\W)+\",\" \", name).strip() # remove special characters and digits\n",
    "    return name\n",
    "\n",
    "\n",
    "\n",
    "def adjust_to_name_length(name, score):\n",
    "    ''' \n",
    "    Returns the difference of name length and average length times .01 \n",
    "    0.01, as 10 is the average, with treaty names ranging between 2 and 25 words (max therefore -/+ .08)\n",
    "    Needed, as short treaty names receive higher matches\n",
    "    However, not implemented in current code, as the adjustment can be done on the csv output file more effectively\n",
    "    '''\n",
    "    \n",
    "    average = 9.73\n",
    "    return score + ((len(name.split()) - average) * .01)\n",
    "    \n",
    "\n",
    "#update treaty names | deletes certain words and numbers, to shorten treaty name length\n",
    "new_treaty_names = []\n",
    "for treaty in treaty_names:\n",
    "    new_treaty_names.append(clean_treaty_names(treaty))\n",
    "treaty_names = new_treaty_names.copy()\n",
    "\n",
    "    \n",
    "@ray.remote #indicates parallel programming\n",
    "def fuzzy_search_treaty_decision(treaty_decision_text, treaty_decision_name, treaty_names, step_size, added_words):\n",
    "    \n",
    "    print(treaty_decision_name)\n",
    "    list_of_outcomes = []\n",
    "    text = clean_text(treaty_decision_text).split() \n",
    "    for treaty in treaty_names: \n",
    "        '''\n",
    "        FUZZY SEARCH\n",
    "        As fuzzysearch doesn't work well if the string length differ widely,\n",
    "        the code loops over different overlaping parts of the text and saves the highest score\n",
    "        '''\n",
    "\n",
    "        high_score = 0\n",
    "        name_len = len(treaty.split()) #see how long the treaty name is\n",
    "        treaty_specific_step_size = (name_len + added_words) // step_size\n",
    "        if treaty_specific_step_size == 0:\n",
    "            treaty_specific_step_size = 1\n",
    "\n",
    "        #use fuzzy search to see the match for the specific part of the text\n",
    "        Search_Query = fuzzyset.FuzzySet()\n",
    "        for num  in range(0, len(text) - 2 - name_len, treaty_specific_step_size): #get the length of the text and get a step size that is according to the name length\n",
    "\n",
    "\n",
    "\n",
    "            Search_Query.add(treaty)\n",
    "            outcome = Search_Query.get(\" \".join(text[num:num + name_len + added_words]))\n",
    "\n",
    "\n",
    "            if outcome != None: #check if the match is None (None will cause error)\n",
    "                if outcome[0][0] > high_score: #check if the current score is larger than the highest score\n",
    "                    high_score = outcome[0][0] #update highscore\n",
    "\n",
    "        #uncomment line below, to adjust for treaty length\n",
    "        #high_score = adjust_to_name_length(treaty, high_score)\n",
    "        list_of_outcomes.append(high_score)\n",
    "        \n",
    "    return treaty_decision_name, list_of_outcomes\n",
    "    \n",
    "    \n",
    "def fuzzy_search_final(step_size = 4, added_words = 2, list_of_subfolders = ['Train Data/']):\n",
    "    \n",
    "    #use cfuzzyset for 15% performance increase\n",
    "    #check documentation \n",
    "    \n",
    "    start_time = time.time()\n",
    "    #add df for saving data\n",
    "    df = pd.DataFrame(columns = treaty_names)\n",
    "    list_of_treaty_decision_texts = []\n",
    "    list_of_treaty_decision_names = []\n",
    "    \n",
    "    for subfolder in list_of_subfolders: #loop over subfolders\n",
    "        list_of_files = listdir(\"Downloads/\" + subfolder) #get all files\n",
    "\n",
    "        for filename in list_of_files: #go through files in subfolder\n",
    "            list_of_outcomes = []\n",
    "\n",
    "\n",
    "\n",
    "            if (filename[-3:] == \"txt\"):  #check if it is a txt document\n",
    "                with open(\"Downloads/\" + subfolder + \"/\" + filename, encoding = \"utf8\") as f:\n",
    "                    document_text = f.readlines()\n",
    "                \n",
    "                list_of_treaty_decision_texts.append(document_text)\n",
    "                list_of_treaty_decision_names.append(filename[:-3])\n",
    "\n",
    "        \n",
    "        #add outcomes to df, save df\n",
    "        ray.shutdown()\n",
    "        ray.init()\n",
    "        list_of_tasks = []\n",
    "        for num in range(len(list_of_treaty_decision_names)):\n",
    "            #add all the texts and names to a list, which will be done parallel in the next step\n",
    "            list_of_tasks.append(fuzzy_search_treaty_decision.remote(list_of_treaty_decision_texts[num], list_of_treaty_decision_names[num], treaty_names, step_size, added_words))\n",
    "        answers = ray.get(list_of_tasks)\n",
    "        \n",
    "        #returns a list of touples with treaty decision name, and a list of the correct values\n",
    "        for answer in answers:    \n",
    "            df.loc[answer[0][:-3]] = answer[1]\n",
    "        df.to_csv(f'Downloads/Fuzzy Search Outcomes Subfolder {subfolder[:-1]}.csv')\n",
    "        \n",
    "        print(f'time FuzzySearch for Subfolder {subfolder[:-1]:} ', time.time() - start_time)\n",
    " \n",
    "\n",
    "    \n",
    "    return 0 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71d82a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pid=29112)\u001b[0m Environmental assessmentinformation system, monitoring and early warning xe2x80x93 Article 12 of the Carpathian Convention.\n",
      " pid=30304)\u001b[0m COP411 Cooperation with the European Union.\n",
      " pid=37840)\u001b[0m A paration for the followup to the Strategic Plan for Biodiversity 20112020 and the Strategic Plan for the Cartagena Protocol on Biosafety 20112020.pdf.\n",
      " pid=17876)\u001b[0m African cherry Prunus africana.\n",
      " pid=30304)\u001b[0m Hexachlorobutadiene.on\n",
      " pid=29112)\u001b[0m Management of Marine Debris.\n",
      " pid=17876)\u001b[0m Outcome of the UNCCD 1st Scientific Conference.pdf.\n",
      " pid=30304)\u001b[0m Preparation for the followup to the Strategic Plan for Biodiversity 20112020.pdf.\n",
      " pid=17876)\u001b[0m Process for aligning national reporting, assessment and review.pdf.\n",
      " pid=30304)\u001b[0m Pyu Ancient Cities Myanmar C 1444.\n",
      " pid=30304)\u001b[0m Review of annexes A and B.pdf.\n",
      " pid=37840)\u001b[0m Submission regarding Swaziland.\n",
      "time FuzzySearch for Subfolder Train Data  764.1514472961426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_search_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f6a3b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_tfidf_features(corpus, max_features=20000, n_gram_range = (1,1), max_df=0.95, min_df=2):\n",
    "    \"\"\" Creates a tf-idf matrix for the `corpus` using sklearn. \"\"\"\n",
    "    tfidf_vectorizor = TfidfVectorizer(decode_error='replace', strip_accents='unicode', analyzer='word',\n",
    "                                       stop_words='english', ngram_range= n_gram_range, max_features=max_features,\n",
    "                                       norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True, vocabulary = None) #or use vocab treaty dict\n",
    "    X = tfidf_vectorizor.fit_transform(corpus)\n",
    "    print('tfidf matrix successfully created.')\n",
    "    return X, tfidf_vectorizor\n",
    "\n",
    "def calculate_similarity(X, vectorizor, queries, top_k=5):\n",
    "    \"\"\" Vectorizes the `query` via `vectorizor` and calculates the cosine similarity of\n",
    "    the `query` and `X` (all the documents) and returns the `top_k` similar documents.\"\"\"\n",
    "\n",
    "    # Vectorize the query to the same length as documents\n",
    "    similarities = []\n",
    "    for query in queries:\n",
    "        query_vec = vectorizor.transform([query])\n",
    "        # Compute the cosine similarity between query_vec and all the documents\n",
    "        cosine_similarities = cosine_similarity(X,query_vec).flatten()\n",
    "        similarities.append(cosine_similarities)\n",
    "        \n",
    "    return similarities\n",
    "\n",
    "\n",
    "def tf_idf_final(max_feature = 100000, n_gram = (1,1), list_of_subfolders = ['Train Data/']):\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(columns = treaty_names)\n",
    "    list_of_treaty_decision_names = []\n",
    "    list_of_treaty_decision_texts = []\n",
    "    \n",
    "    for subfolder in list_of_subfolders: #loop over subfolders\n",
    "        list_of_files = listdir(\"Downloads/\" + subfolder) #get all files\n",
    "\n",
    "        for filename in list_of_files: #go through files in subfolder\n",
    "            \n",
    "            if (filename[-3:] == \"txt\"):  #check if it is a txt document\n",
    "                with open(\"Downloads/\" + subfolder + \"/\" + filename, encoding = \"utf8\") as f:\n",
    "                    document_text = f.readlines()\n",
    "                    \n",
    "               \n",
    "                \n",
    "                list_of_treaty_decision_names.append(filename)\n",
    "                doc_text = clean_text(document_text)\n",
    "                list_of_treaty_decision_texts.append(doc_text)\n",
    "                \n",
    "        start_time = time.time()        \n",
    "        '''\n",
    "        VERSION 0: TF-IDF Matrix\n",
    "        Super Quick, However, not very accurate so far\n",
    "        '''\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        X,v = create_tfidf_features(list_of_treaty_decision_texts, max_features = max_feature, n_gram_range = n_gram)  \n",
    "        features = v.get_feature_names()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        similarities = calculate_similarity(X, v, treaty_names)\n",
    "        similarities = np.array(similarities).T\n",
    "        for num, sims in enumerate(similarities):\n",
    "            list_sim = []\n",
    "            for sim in sims:\n",
    "                list_sim.append(float(sim))\n",
    "            \n",
    "            df.loc[list_of_treaty_decision_names[num][:-3]] = list_sim\n",
    "            print('filename: ', list_of_treaty_decision_names[num])\n",
    "            print('Successful')\n",
    "            print()\n",
    "        df.to_csv(f'Downloads/TF-IDF Search Outcomes Subfolder {subfolder[:-1]}.csv')\n",
    "        \n",
    "\n",
    "\n",
    "        #overall_score.append(score)\n",
    "        #attributes.append((max_feature, n_gram))\n",
    "        #return overall_score, attributes\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6307d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf matrix successfully created.\n",
      "filename:  A paration for the followup to the Strategic Plan for Biodiversity 20112020 and the Strategic Plan for the Cartagena Protocol on Biosafety 20112020.pdf.txt\n",
      "Successful\n",
      "\n",
      "filename:  African cherry Prunus africana.txt\n",
      "Successful\n",
      "\n",
      "filename:  COP411 Cooperation with the European Union.txt\n",
      "Successful\n",
      "\n",
      "filename:  Environmental assessmentinformation system, monitoring and early warning xe2x80x93 Article 12 of the Carpathian Convention.txt\n",
      "Successful\n",
      "\n",
      "filename:  Hexachlorobutadiene.txt\n",
      "Successful\n",
      "\n",
      "filename:  Management of Marine Debris.txt\n",
      "Successful\n",
      "\n",
      "filename:  Outcome of the UNCCD 1st Scientific Conference.pdf.txt\n",
      "Successful\n",
      "\n",
      "filename:  Preparation for the followup to the Strategic Plan for Biodiversity 20112020.pdf.txt\n",
      "Successful\n",
      "\n",
      "filename:  Process for aligning national reporting, assessment and review.pdf.txt\n",
      "Successful\n",
      "\n",
      "filename:  Pyu Ancient Cities Myanmar C 1444.txt\n",
      "Successful\n",
      "\n",
      "filename:  Review of annexes A and B.pdf.txt\n",
      "Successful\n",
      "\n",
      "filename:  Submission regarding Swaziland.txt\n",
      "Successful\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0583041d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
